---
title: "Prediction of the quality of physical exercise on the basis of accelerometer data"
author: "Moritz Körber"
test: "`r Sys.setlocale(locale = 'English')`"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  html_document:
    keep_md: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Background

Going to the gym, whether to boost your health, to lose weight, or simply because it is fun, is certainly a worthwile activity. [FiveThirtyEight](https://fivethirtyeight.com/features/its-easier-than-ever-to-get-the-recommended-amount-of-exercise/) recently reported that according to the latest *Physical Activity Guidelines for Americans*, every form of activity counts. However, if you have eager goals, not only quantity but also quality and being efficient matters. In this project, I predict whether a certain exercise, a barbell lift, was well or sloppily executed on the basis of data obtained from accelerometers on the belt, forearm, arm, and dumbell of six participants. The participants performed the exercises correctly and incorrectly in five different ways. The idea for this analysis stems from the Coursera course *Practical Machine Learning* by Johns Hopkins University. The data for this project come from [http://groupware.les.inf.puc-rio.br/har](http://groupware.les.inf.puc-rio.br/har).

# Analysis

The first step is to load all required packages:

```{r message=FALSE, warning=FALSE}
library("dplyr")
library("caret")
library("mlr")
library("visdat")
library("data.table")
```

Then I load the data set with the speedy `fread()` function of `data.table` and set a seed for reproducibility.

```{r}
df <- fread("1_data/pml-training.csv", na.strings = c("NA", "NaN", "", "#DIV/0!"), drop = 1)

set.seed(31)
```

# 1. Inspect the target variable
First, I am looking at the target variable `classe`. What type is it, how many "classes" are there in the data set?

```{r}
glimpse(df$classe)
```

Since it is a categorical variable, the analysis represents a classification problem. Hence, an algorithm like logistic regression or random forest is suitable. Let's check if the classes are balanced:

```{r}
ggplot(df, aes(x = classe)) +
  geom_bar(stat = "count", fill = "#005293") +
  theme_classic()

count(df, classe)
```

The classes seem to be pretty balanced. This will make cross-validation and performance evaluation a bit easier.

# 2. Clean the data set

First, I remove obviously non-predictive features such as `user_name`.
```{r}
df %>% select(-c(raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp, user_name)) -> df
```

Next, I am looking for zero variance/near zero variance features and remove them afterwards.
```{r}
nzv <- nearZeroVar(df)
df <- df[ , -..nzv]

df <- removeConstantFeatures(as.data.frame(df), perc = .02)
```

The same goes for features that contain mostly NAs. I chose a cut-off of 97.5%, i.e. I remove a feature if 97.5% or more of its cases are NA. Before I mindlessly discard these predictors, I am having a closer look at them – cut-offs are comfortable but may be nonsensical sometimes.

```{r}
df %>%
  select(everything()) %>%
  summarise_all(funs(sum(is.na(.)) / length(.))) -> p

# check these variables
vis_miss(df[which(p > 0.975)],
  sort_miss = TRUE, warn_large_data = F
)

# remove them if sensible
df[which(p > 0.975)] <- NULL
```

Highly correlated features contain mostly the same information. Hence, keeping both does not have any value at best and hurts fitting the model at worst. I am looking for features that are highly correlated for this reason and discard them if it seems sensible. 

```{r}
nums <- select_if(df, is.numeric)
descrCor <- cor(nums)

highCorr <- sum(na.omit(abs(descrCor[upper.tri(descrCor)])) >= .98)

na.omit(descrCor[upper.tri(descrCor)])[which(na.omit(abs(descrCor[upper.tri(descrCor)])) >= .98, arr.ind = TRUE)]

which(na.omit(abs(descrCor)) >= .98 & na.omit(abs(descrCor)) < 1, arr.ind = TRUE)

findCorrelation(na.omit(descrCor), cutoff = .98, verbose = T, exact = T, names = T)
```

There are two variables with a very high correlation. I will leave them in the dataset for this time, but performing the rest of the analysis without them is certainly an interesting alley to go down.

The `mlr` package provides a similar, more concise function:
```{r}
findLinearCombos(nums)
```

# 3. Visualize the data

Time to take a step back and to have a look at the result of these cleaning steps. 

```{r}
vis_dat(df, warn_large_data = F)
```

Let's see if there are mixed data types within a single feature.
```{r}
vis_guess(df)
```

Plotting the relationship of the features with the target, `classe`, yields a first glimpse on potentially meaningful features. 
```{r eval = FALSE}
nums <- unlist(lapply(df, is.numeric))
featurePlot(x = as.data.frame(df)[nums], y = df$classe, plot = "strip")
```
<figure>
  <img src="images/plot_features.png"/>
</figure>

Looks fine, let's save the progress! We can now tinker around with a clean data set and can always return back to this state.
```{r eval = FALSE}
saveRDS(df, "1_data/cleaned_data.rds")
```

# 4. Training
Since the training is a bit computational heavy, I outsourced this step and brought an Amazon AWS EC2 t2.2xlarge instance into play. I performed the training steps shown down below on this instance and then transferred the results to my own machine. Here are the details on its environment:
```{r echo = FALSE}
readRDS("1_data/sessionInfo.rds")
```

## Task
What do we want our machine to learn? I first need to specify the data set and the target for prediction, here `classe`. In the `mlr` package, this is done by defining a task:
```{r}
task <- makeClassifTask(id = "fitness.tracker", data = df, target = "classe")
```

## Resampling
I chose to use a nested cross-validation strategy with a 5-fold inner cross-validation plan and a 3-fold outer cross-validation plan. Following the 3-fold outer cross-valdation plan, the data set is first split into 3 folds. Evaluation of the test error and hyperparameter tuning are performed using two of these folds by 5-fold cross-validation, the inner cross-validation plan:
```{r}
rdesc.inner <- makeResampleDesc("CV", iters = 5)
```

The best parameter combination is then evaluated against the remaining fold of the 3-fold outer cross-validation plan:
```{r}
rdesc.outer <- makeResampleDesc(method = "CV", iters = 3)
resample.instance.outer <- makeResampleInstance(desc = rdesc.outer, task = task)
```

The [mlr homepage](https://mlr.mlr-org.com/index.html) provides a great graphical representation of this strategy [here](https://mlr.mlr-org.com/articles/tutorial/nested_resampling.html).

<figure style="width: 75%; margin: 0 auto;">
  <img src="images/nested_resampling.png"/>
  <figcaption style="width: 100%; margin: 0 auto;">Note that the illustrated strategy uses a different number of folds. </figcaption>
</figure>

## Measures
Since it is a classification problem and the classes seem to be balanced, I chose the mean misclassification error to evaluate the learners' performance. Other metrics, such as accuracy or F1-score, would also have been suitable and are easy to add if necessary.
```{r}
measures <- list(mmce)
```

## Learners
I compare the performance of three different learners: a random forest, XGBoost (a gradient boosting algorithm), and a ranger (a fast implementation of random forests). Each learner's hyperparameters are tuned in the cross-validation process. For preprocessing, I center and scale the features. Here are the leaners' instantiations:

### Random forest
```{r eval = FALSE}
lrn.rndforest <- makePreprocWrapperCaret("classif.randomForest", ppc.center = T, ppc.scale = T)

ps.rndforest <- makeParamSet(
  makeIntegerParam("ntree", lower = 100, upper = 1000),
  makeIntegerParam("mtry", lower = 5, upper = 20)
)

tune.ctrl.rndforest <- makeTuneControlRandom(maxit = 30)

tuned.lrn.rndforest <- makeTuneWrapper(lrn.rndforest,
  par.set = ps.rndforest,
  resampling = rdesc.inner,
  control = tune.ctrl.rndforest
)
```

### XGBoost
```{r}
lrn.xgboost <- makePreprocWrapperCaret("classif.xgboost", ppc.center = T, ppc.scale = T)

ps.xgboost <- makeParamSet(
  makeNumericParam("eta", lower = 0, upper = 0.5),
  makeNumericParam("colsample_bytree", lower = 0.5, upper = 0.9),
  makeNumericParam("gamma", lower = 0, upper = 2),
  makeIntegerParam("max_depth", lower = 4, upper = 10),
  makeIntegerParam("nrounds", lower = 500, upper = 1500)
)

tune.ctrl.xgboost <- makeTuneControlRandom(maxit = 30)

tuned.lrn.xgboost <- makeTuneWrapper(lrn.xgboost,
  par.set = ps.xgboost,
  resampling = rdesc.inner,
  control = tune.ctrl.xgboost
)
```

### Ranger
```{r eval = FALSE}
lrn.ranger <- makePreprocWrapperCaret("classif.ranger", ppc.center = T, ppc.scale = T)
```

## Benchmark
Let's see how well each learner does!
```{r eval = FALSE}
bm <- benchmark(
  learners = list(
    tuned.lrn.rndforest,
    tuned.lrn.xgboost,
    lrn.ranger
  ),
  tasks = task,
  resamplings = resample.instance.outer,
  measures = measures
)

bm
```

```{r echo = FALSE}
bm <- readRDS("1_data/bm.rds")
```

```{r eval = FALSE}
plotBMRBoxplots(bm)
```

```{r echo = FALSE}
readRDS("1_data/bmplot.rds")
```

The XGBoost algorithm seems to do the best job here. Thus, I concentrate on this learner and go on with extended hyperparameter tuning. Next, I train the final model on the complete training data set and discard all others but the best performing hyperparameter set. 

# 4. Train final model
```{r  eval = FALSE}
model <- mlr::train(learner = tuned.lrn.xgboost, task = task)

saveRDS(model, "1_data/model.rds")
```

```{r include = FALSE}
model <- readRDS("1_data/model.rds")
```

# 5. Predict the test data
The last step of this analysis is to predict a small, completely new, and independent test set. First step again is to load the test data.

## Load and prepare test data
```{r}
testing <- read.csv("1_data/pml-testing.csv", na.strings = c("NA", "NaN", "", "#DIV/0!"), row.names = 1)

# make sure that they have the same columns (except the target)
df %>%
  select(-classe) %>%
  colnames() -> vars

testing <- testing[vars]
```

## Prediction
I use the model trained above to predict the 20 cases in this final test set. 
```{r}
pred <- predict(model, newdata = testing)
```

Let's see how we did here:
```{r echo = FALSE}
true_labels <- read.csv2("5_blogpost/data/true_labels.csv")

results <- data.frame(pred, true_labels)

results$correct_prediction <- ifelse(results$response == results$true_labels, "yes", "no")

results
```

Seems like the learner did a great job! It predicted every case correctly. Yay! :)